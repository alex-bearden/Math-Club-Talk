{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8367d8b0",
   "metadata": {},
   "source": [
    "We use character-level language modeling to generate text similar to math phrases scraped from the [Encylopedia of Mathematics](https://encyclopediaofmath.org/wiki/Special:AllPages).\n",
    "\n",
    "This is heavily based on Chapter 15 of **Machine Learning with PyTorch and Scikit-Learn** by Raschka, Liu, and Mirjalili. The main technical difference is that I did not use an embedding as the first layer of the network, but instead used a one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "19d21080",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5bdc7f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Length: 206528\n",
      "Unique Characters: 97\n"
     ]
    }
   ],
   "source": [
    "with open('math_phrases_final.txt', 'r', encoding=\"utf8\") as fp:\n",
    "    text = fp.read()\n",
    "phrase_list = text.split('\\n')\n",
    "phrase_list = phrase_list[:-1]\n",
    "random.seed(42)\n",
    "random.shuffle(phrase_list)\n",
    "text = '\\n'.join(phrase_list)\n",
    "char_set = set(text)\n",
    "print('Total Length:', len(text))\n",
    "print('Unique Characters:', len(char_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f6668115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text encoded shape: (206528,) \n",
      "\n",
      "Moore space ==> Encoded: [32 61 61 64 51  1 65 62 47 49 51] \n",
      "\n",
      "[23 47 64 59 61 55 65  8  1 26 51 61 64 53 51 65] ==> Decoded: Darmois, Georges\n"
     ]
    }
   ],
   "source": [
    "chars_sorted = sorted(char_set)\n",
    "char2int = {ch : i for i, ch in enumerate(chars_sorted)}\n",
    "char_array = np.array(chars_sorted)\n",
    "text_encoded = np.array([char2int[ch] for ch in text], dtype=np.int32)\n",
    "print('Text encoded shape:', text_encoded.shape, '\\n')\n",
    "print(text[:11], '==> Encoded:', text_encoded[:11], '\\n')\n",
    "print(text_encoded[12:28], '==> Decoded:', ''.join(char_array[text_encoded[12:28]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "29005a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "seq_length = 40\n",
    "chunk_size = seq_length + 1\n",
    "text_chunks = [text_encoded[i : i + chunk_size] for i in range(len(text_encoded) - chunk_size)]\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text_chunks):\n",
    "        self.text_chunks = text_chunks\n",
    "    def __len__(self):\n",
    "        return len(self.text_chunks)\n",
    "    def __getitem__(self, idx):\n",
    "        text_chunk = self.text_chunks[idx]\n",
    "        return text_chunk[:-1].long(), text_chunk[1:].long()\n",
    "seq_dataset = TextDataset(torch.tensor(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "63cde15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Input (x):  'Moore space\\nDarmois, Georges\\nSymplectic '\n",
      "Target (y):  'oore space\\nDarmois, Georges\\nSymplectic s'\n",
      "\n",
      " Input (x):  'oore space\\nDarmois, Georges\\nSymplectic s'\n",
      "Target (y):  'ore space\\nDarmois, Georges\\nSymplectic sp'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, (seq, target) in enumerate(seq_dataset):\n",
    "    print(' Input (x): ', repr(''.join(char_array[seq])))\n",
    "    print('Target (y): ', repr(''.join(char_array[target])))\n",
    "    print()\n",
    "    if i == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d43a318b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 64\n",
    "torch.manual_seed(1)\n",
    "seq_dl = DataLoader(seq_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a4705f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size):\n",
    "        super().__init__()\n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "        self.rnn = nn.LSTM(vocab_size, rnn_hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(rnn_hidden_size, vocab_size)\n",
    "    def forward(self, x, hidden, cell):\n",
    "        out = torch.nn.functional.one_hot(x, num_classes=97).float().unsqueeze(1)\n",
    "        out, (hidden, cell) = self.rnn(out, (hidden, cell))\n",
    "        out = self.fc(out).reshape(out.size(0), -1)\n",
    "        return out, hidden, cell\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
    "        cell = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cf92aeb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (rnn): LSTM(97, 512, batch_first=True)\n",
       "  (fc): Linear(in_features=512, out_features=97, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(char_array)\n",
    "embed_dim = vocab_size\n",
    "rnn_hidden_size = 512\n",
    "torch.manual_seed(1)\n",
    "model = RNN(vocab_size, embed_dim, rnn_hidden_size)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a8017d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5c636c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 4.5786\n",
      "Time elapsed:  0:00:00.605819\n",
      "Epoch 100 loss: 3.2197\n",
      "Time elapsed:  0:01:01.830133\n",
      "Epoch 200 loss: 2.7251\n",
      "Time elapsed:  0:02:00.716220\n",
      "Epoch 300 loss: 2.4723\n",
      "Time elapsed:  0:03:01.245320\n",
      "Epoch 400 loss: 2.2895\n",
      "Time elapsed:  0:04:02.286744\n",
      "Epoch 500 loss: 2.2854\n",
      "Time elapsed:  0:05:08.924041\n",
      "Epoch 600 loss: 2.1464\n",
      "Time elapsed:  0:06:11.047495\n",
      "Epoch 700 loss: 2.1592\n",
      "Time elapsed:  0:07:17.325632\n",
      "Epoch 800 loss: 2.1319\n",
      "Time elapsed:  0:08:17.632523\n",
      "Epoch 900 loss: 2.0456\n",
      "Time elapsed:  0:09:17.320054\n",
      "Epoch 1000 loss: 1.9324\n",
      "Time elapsed:  0:10:17.900096\n",
      "Epoch 1100 loss: 1.8673\n",
      "Time elapsed:  0:11:17.895343\n",
      "Epoch 1200 loss: 1.9153\n",
      "Time elapsed:  0:12:18.378661\n",
      "Epoch 1300 loss: 1.8174\n",
      "Time elapsed:  0:13:19.031256\n",
      "Epoch 1400 loss: 1.7354\n",
      "Time elapsed:  0:14:19.073976\n",
      "Epoch 1500 loss: 1.7110\n",
      "Time elapsed:  0:15:22.502012\n",
      "Epoch 1600 loss: 1.7134\n",
      "Time elapsed:  0:16:26.297331\n",
      "Epoch 1700 loss: 1.6507\n",
      "Time elapsed:  0:17:24.587615\n",
      "Epoch 1800 loss: 1.5146\n",
      "Time elapsed:  0:18:24.005113\n",
      "Epoch 1900 loss: 1.4526\n",
      "Time elapsed:  0:19:28.705621\n",
      "Epoch 2000 loss: 1.5321\n",
      "Time elapsed:  0:20:29.431453\n",
      "Epoch 2100 loss: 1.5481\n",
      "Time elapsed:  0:21:30.134501\n",
      "Epoch 2200 loss: 1.5033\n",
      "Time elapsed:  0:22:45.974120\n",
      "Epoch 2300 loss: 1.4315\n",
      "Time elapsed:  0:24:24.156284\n",
      "Epoch 2400 loss: 1.3938\n",
      "Time elapsed:  0:25:49.989346\n",
      "Epoch 2500 loss: 1.2997\n",
      "Time elapsed:  0:27:01.108391\n",
      "Epoch 2600 loss: 1.3926\n",
      "Time elapsed:  0:28:17.859537\n",
      "Epoch 2700 loss: 1.3281\n",
      "Time elapsed:  0:29:38.773463\n",
      "Epoch 2800 loss: 1.2434\n",
      "Time elapsed:  0:30:49.689008\n",
      "Epoch 2900 loss: 1.2932\n",
      "Time elapsed:  0:32:08.120048\n",
      "Epoch 3000 loss: 1.2663\n",
      "Time elapsed:  0:33:26.851851\n",
      "Epoch 3100 loss: 1.1722\n",
      "Time elapsed:  0:35:01.282000\n",
      "Epoch 3200 loss: 1.2255\n",
      "Time elapsed:  0:36:10.536935\n",
      "Epoch 3300 loss: 1.2327\n",
      "Time elapsed:  0:37:24.974384\n",
      "Epoch 3400 loss: 1.1524\n",
      "Time elapsed:  0:38:28.771661\n",
      "Epoch 3500 loss: 1.1779\n",
      "Time elapsed:  0:39:35.591546\n",
      "Epoch 3600 loss: 1.1826\n",
      "Time elapsed:  0:41:03.860078\n",
      "Epoch 3700 loss: 1.1323\n",
      "Time elapsed:  0:42:25.800326\n",
      "Epoch 3800 loss: 1.0995\n",
      "Time elapsed:  0:43:24.329821\n",
      "Epoch 3900 loss: 1.1244\n",
      "Time elapsed:  0:44:23.225698\n",
      "Epoch 4000 loss: 1.0438\n",
      "Time elapsed:  0:45:30.820644\n",
      "Epoch 4100 loss: 1.0567\n",
      "Time elapsed:  0:46:44.009398\n",
      "Epoch 4200 loss: 1.0554\n",
      "Time elapsed:  0:47:53.417079\n",
      "Epoch 4300 loss: 1.0885\n",
      "Time elapsed:  0:48:56.988577\n",
      "Epoch 4400 loss: 1.0057\n",
      "Time elapsed:  0:50:07.005770\n",
      "Epoch 4500 loss: 1.0365\n",
      "Time elapsed:  0:51:13.873972\n",
      "Epoch 4600 loss: 0.9926\n",
      "Time elapsed:  0:52:17.282183\n",
      "Epoch 4700 loss: 1.0170\n",
      "Time elapsed:  0:53:15.561916\n",
      "Epoch 4800 loss: 1.0216\n",
      "Time elapsed:  0:54:35.644841\n",
      "Epoch 4900 loss: 0.9815\n",
      "Time elapsed:  0:55:41.239513\n",
      "Epoch 5000 loss: 0.9247\n",
      "Time elapsed:  0:56:47.235241\n",
      "Epoch 5100 loss: 0.9200\n",
      "Time elapsed:  0:57:56.615739\n",
      "Epoch 5200 loss: 0.9041\n",
      "Time elapsed:  0:59:11.031671\n",
      "Epoch 5300 loss: 0.8675\n",
      "Time elapsed:  1:00:29.225956\n",
      "Epoch 5400 loss: 0.9226\n",
      "Time elapsed:  1:01:39.478034\n",
      "Epoch 5500 loss: 0.8623\n",
      "Time elapsed:  1:02:42.116638\n",
      "Epoch 5600 loss: 0.8648\n",
      "Time elapsed:  1:03:44.884632\n",
      "Epoch 5700 loss: 0.8375\n",
      "Time elapsed:  1:04:44.456367\n",
      "Epoch 5800 loss: 0.8883\n",
      "Time elapsed:  1:05:46.223498\n",
      "Epoch 5900 loss: 0.8349\n",
      "Time elapsed:  1:06:51.693329\n",
      "Epoch 6000 loss: 0.8118\n",
      "Time elapsed:  1:07:56.901536\n",
      "Epoch 6100 loss: 0.7966\n",
      "Time elapsed:  1:09:01.311483\n",
      "Epoch 6200 loss: 0.8064\n",
      "Time elapsed:  1:10:05.236370\n",
      "Epoch 6300 loss: 0.8174\n",
      "Time elapsed:  1:11:11.477209\n",
      "Epoch 6400 loss: 0.7539\n",
      "Time elapsed:  1:12:26.719429\n",
      "Epoch 6500 loss: 0.8232\n",
      "Time elapsed:  1:13:37.374855\n",
      "Epoch 6600 loss: 0.7351\n",
      "Time elapsed:  1:14:53.027421\n",
      "Epoch 6700 loss: 0.7753\n",
      "Time elapsed:  1:16:24.654804\n",
      "Epoch 6800 loss: 0.7559\n",
      "Time elapsed:  1:17:38.978080\n",
      "Epoch 6900 loss: 0.7898\n",
      "Time elapsed:  1:19:00.426624\n",
      "Epoch 7000 loss: 0.7470\n",
      "Time elapsed:  1:20:08.168029\n",
      "Epoch 7100 loss: 0.7408\n",
      "Time elapsed:  1:21:08.132332\n",
      "Epoch 7200 loss: 0.7331\n",
      "Time elapsed:  1:22:08.675015\n",
      "Epoch 7300 loss: 0.7063\n",
      "Time elapsed:  1:23:20.614911\n",
      "Epoch 7400 loss: 0.7186\n",
      "Time elapsed:  1:24:41.485509\n",
      "Epoch 7500 loss: 0.6874\n",
      "Time elapsed:  1:25:43.312152\n",
      "Epoch 7600 loss: 0.7001\n",
      "Time elapsed:  1:26:46.908628\n",
      "Epoch 7700 loss: 0.6711\n",
      "Time elapsed:  1:27:48.826875\n",
      "Epoch 7800 loss: 0.7251\n",
      "Time elapsed:  1:28:50.973233\n",
      "Epoch 7900 loss: 0.6943\n",
      "Time elapsed:  1:29:56.297974\n",
      "Epoch 8000 loss: 0.6894\n",
      "Time elapsed:  1:30:59.206030\n",
      "Epoch 8100 loss: 0.6688\n",
      "Time elapsed:  1:31:58.300699\n",
      "Epoch 8200 loss: 0.6593\n",
      "Time elapsed:  1:32:58.804861\n",
      "Epoch 8300 loss: 0.6337\n",
      "Time elapsed:  1:33:59.047716\n",
      "Epoch 8400 loss: 0.6563\n",
      "Time elapsed:  1:35:00.152793\n",
      "Epoch 8500 loss: 0.6637\n",
      "Time elapsed:  1:36:00.929681\n",
      "Epoch 8600 loss: 0.6440\n",
      "Time elapsed:  1:37:00.498313\n",
      "Epoch 8700 loss: 0.6351\n",
      "Time elapsed:  1:38:00.798556\n",
      "Epoch 8800 loss: 0.5925\n",
      "Time elapsed:  1:39:07.011725\n",
      "Epoch 8900 loss: 0.6438\n",
      "Time elapsed:  1:40:10.024706\n",
      "Epoch 9000 loss: 0.6249\n",
      "Time elapsed:  1:41:09.947449\n",
      "Epoch 9100 loss: 0.6166\n",
      "Time elapsed:  1:42:15.715587\n",
      "Epoch 9200 loss: 0.5841\n",
      "Time elapsed:  1:43:21.845572\n",
      "Epoch 9300 loss: 0.5756\n",
      "Time elapsed:  1:44:25.336858\n",
      "Epoch 9400 loss: 0.5808\n",
      "Time elapsed:  1:45:30.676497\n",
      "Epoch 9500 loss: 0.5684\n",
      "Time elapsed:  1:46:36.902792\n",
      "Epoch 9600 loss: 0.5650\n",
      "Time elapsed:  1:47:47.115074\n",
      "Epoch 9700 loss: 0.6036\n",
      "Time elapsed:  1:48:54.035806\n",
      "Epoch 9800 loss: 0.5893\n",
      "Time elapsed:  1:49:56.065693\n",
      "Epoch 9900 loss: 0.5937\n",
      "Time elapsed:  1:50:58.952082\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "start_time = datetime.now()\n",
    "\n",
    "num_epochs = 10000\n",
    "torch.manual_seed(1)\n",
    "for epoch in range(num_epochs):\n",
    "    hidden, cell = model.init_hidden(batch_size)\n",
    "    seq_batch, target_batch = next(iter(seq_dl))\n",
    "    optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    for c in range(seq_length):\n",
    "        pred, hidden, cell = model(seq_batch[:, c], hidden, cell)\n",
    "        loss += loss_fn(pred, target_batch[:, c])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss = loss.item() / seq_length\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch} loss: {loss:.4f}')\n",
    "        print('Time elapsed: ', datetime.now() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3437eb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions.categorical import Categorical\n",
    "def sample(model, starting_str, len_generated_text=5000, scale_factor=1.0):\n",
    "    encoded_input = torch.tensor([char2int[s] for s in starting_str])\n",
    "    encoded_input = torch.reshape(encoded_input, (1, -1))\n",
    "    generated_str = starting_str\n",
    "    model.eval()\n",
    "    hidden, cell = model.init_hidden(1)\n",
    "    for c in range(len(starting_str)-1):\n",
    "        _, hidden, cell = model(encoded_input[:, c].view(1), hidden, cell)\n",
    "    last_char = encoded_input[:, -1]\n",
    "    for i in range(len_generated_text):\n",
    "        logits, hidden, cell = model(last_char.view(1), hidden, cell)\n",
    "        logits = torch.squeeze(logits, 0)\n",
    "        scaled_logits = logits * scale_factor\n",
    "        m = Categorical(logits=scaled_logits)\n",
    "        last_char = m.sample()\n",
    "        generated_str += str(char_array[last_char])\n",
    "    return generated_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eafb62c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algebraic equality of a complex varistictest\n",
      "Tirchequiforbiling operator\n",
      "Ergodic shmorigh\n",
      "Frotuet-Dönitolly space\n",
      "Prepistul-Lyoption\n",
      "Cohm-group\n",
      "Bunnagow-Lagrange method\n",
      "Dios a coation\n",
      "Franti-gemeeribility transformation\n",
      "Singular semi-group\n",
      "Vinogradov-Goldbach theorem\n",
      "Schur method\n",
      "One-sided and linesd\n",
      "Sperlew operator\n",
      "Pro-barr–Doxine laction\n",
      "Conjugate class of functions, extremal equation\n",
      "Approximation\n",
      "Bourled brank\n",
      "Axtric group\n",
      "Rebuture condition\n",
      "De Rham\n",
      "Hodfordject method\n",
      "Almost-periodic functional\n",
      "Newton-babid factorization of\n",
      "Hilbert'-Skeröshhey zycomp(sia closure\n",
      "Katon formula\n",
      "Supersal\n",
      "Recurring series\n",
      "Primitive representation of andidion\n",
      "Boxes-Heaing representation\n",
      "Protedenoso, method of a complex variation problem\n",
      "Game on property\n",
      "Decision ring\n",
      "Distance-Teurn-Granse semier\n",
      "K-sacaukal phine\n",
      "Doudingroup\n",
      "Giassdary\n",
      "Axtoncical ideal surface\n",
      "Optimal numbers\n",
      "Poisson distribution\n",
      "Schledeam group\n",
      "Witt chain\n",
      "Problème des group\n",
      "Regular algebra of a group\n",
      "Cubically class\n",
      "Locally convex space\n",
      "Absslunc spectra\n",
      "Asymptotic polynomial\n",
      "Jordan matrix\n",
      "Homoconjugate\n",
      "Tation of a Lie group\n",
      "Uncouneacl structurestics\n",
      "Irner\n",
      "Sussticatzzzit victor\n",
      "Integral coasticity\n",
      "Vietoris (of functions on mappings, Viscal theorem\n",
      "Pootion sequence\n",
      "Collineation\n",
      "Innimomboldidiation\n",
      "Singular correlation coedlances\n",
      "Quasi-character\n",
      "Locally structure group\n",
      "Sperors of a stapis in\n",
      "Bessel processes\n",
      "Aluason ascrepa\n",
      "Abalia correlation coefficient\n",
      "Euslins theorem\n",
      "Krasch of an algebraic function\n",
      "Dissonn-atomination\n",
      "Eineletery differential equation\n",
      "Poisson frection\n",
      "Smirnov derivative\n",
      "Jeferting relation\n",
      "Gauss-Lucas theorem\n",
      "Brownian flow\n",
      "Klein–Gorzung theorem\n",
      "Cramer theorem\n",
      "Non-Abel congruence\n",
      "Shakev continuum\n",
      "Baer sphere\n",
      "Dynamics of surfaces\n",
      "Mulli theorem\n",
      "Chrastavate\n",
      "Tor\n",
      "Illembert variational method\n",
      "Shannon sampling plana\n",
      "Norusan inequality\n",
      "Bianchi worf\n",
      "Non-stability theory\n",
      "Integral test\n",
      "Storpho-madkey of a space\n",
      "Kousder-fantorrad point\n",
      "Ober-everytic\n",
      "Mataulay operator\n",
      "Overgence in coonson theorem\n",
      "Pre-modulo\n",
      "Swierbrims in alternation\n",
      "Benjams–Lawner-H-anstach\n",
      "BellkAnn-Masheu nets\n",
      "Ornel–Condelpale domain\n",
      "Derivatermale radical\n",
      "Monokov exact operator\n",
      "Riemann–Hilbert problem\n",
      "SRaneal holotons, negeriop\n",
      "Quantum complex discontinuum\n",
      "Biscatoford, Masile\n",
      "Hölder cocressive\n",
      "Iccuser summation method\n",
      "Tate motifier\n",
      "Poisson trape\n",
      "Small mathom theory)\n",
      "Discontinuous functional\n",
      "Aczevis–Fight function\n",
      "Gel'fand representation of ordinarylipic\n",
      "Construchito hast approximation\n",
      "Klain space\n",
      "Eltce\n",
      "Borel skave theorem\n",
      "Luzin-Privalovi theorem\n",
      "Adine links, quasi-group\n",
      "Polthedo-prime varceset assusting and quasi-logarithmic residue\n",
      "Rabi aussy(2n\n",
      "Ratoi algebra\n",
      "Riemannian space\n",
      "Nine wigh nese, curve of the\n",
      "Projertine probability theory\n",
      "Szegö limit theorems\n",
      "Rad dellypponed\n",
      "Lambert space\n",
      "E-M-fucclic\n",
      "Dimension of a manifold\n",
      "Euler–nocial function\n",
      "Prüfer surface\n",
      "Eigen oscillation\n",
      "Definite kervel\n",
      "Knotthnsko–dimensional manifold\n",
      "Approach space\n",
      "Charle\n",
      "Two-beylince scheme\n",
      "Wiener–Ireal inequality\n",
      "Prenersing operator\n",
      "Leay subvariets\n",
      "Vinogradov est\n",
      "Marton method\n",
      "Analytic manifold\n",
      "Euler transformation\n",
      "Zemurato compactification\n",
      "Second order\n",
      "Affining oster\n",
      "Close singular point\n",
      "Extension-of\n",
      "Schoded axioms\n",
      "Hartygn vensol\n",
      "Tircherdor hypothesis\n",
      "Carathéodory–Fejér problem\n",
      "Hanne-iseming\n",
      "Density model\n",
      "Leog-line with characteristic\n",
      "Primary differential equation, partial, oblique derivation\n",
      "Uniformizability\n",
      "Uniform singular integory\n",
      "Non-linear statility\n",
      "Permigatoo bise\n",
      "Issclited abructer\n",
      "Masp\n",
      "Arg\n",
      "Diasc ergoreh\n",
      "Arrangemenu system\n",
      "Stinfeg number\n",
      "Remuracto iverais\n",
      "Tanesy algebra calculus\n",
      "Maximal transformation\n",
      "Connected caraly\n",
      "Schrödingence, mathematical-trienta edect\n",
      "Regular prime number theorem\n",
      "Awtriod-Dippler problem\n",
      "Dedekind variation algebra\n",
      "Eugr duclion\n",
      "Helly's trinnneral topological\n",
      "Complexily nuterbyling\n",
      "P-adic ligit\n",
      "Mixed boundary in potential transform\n",
      "Gane series\n",
      "Constructive limit\n",
      "Dirichlet algebra\n",
      "Acceleration of domain of a stability theory\n",
      "Synthesian group\n",
      "Staristum, fundle\n",
      "Webe vector\n",
      "Regular random variableT\n",
      "Schur theorem\n",
      "Prodontwise\n",
      "Ajoipolation of functions\n",
      "Leibniz criterion\n",
      "Ond-discrete distribution\n",
      "Flag function\n",
      "Haerl remeari dynamics)\n",
      "A-group\n",
      "Steffenbelger test\n",
      "D'Alembert operator\n",
      "Lie algebra, stau theory\n",
      "Caratheorovicis\n",
      "Men-Whioner scheme\n",
      "D'Alembert–Subburce reluarboid\n",
      "AD-Kielgrboden derivation\n",
      "Square-variety of algebra\n",
      "Piraudde dariat)\n",
      "Makn series\n",
      "Statistical geometry)\n",
      "Wil\n",
      "Polamone norma\n",
      "Group closure\n",
      "Inality test\n",
      "Matrix Hillex graph\n",
      "Lower ligy ordered set\n",
      "Indiconne closure\n",
      "Traps, combination\n",
      "Planch\n",
      "Sylthetial measure\n",
      "Harbynskay square\n",
      "Merten algebra\n",
      "D0L-sequence\n",
      "Brayamingor projector\n",
      "Bikeanov theorem\n",
      "Pielet–nur inek\n",
      "Euclidean space\n",
      "Sortenkeve–dynami newarger problem\n",
      "Commutants, method of\n",
      "Banosi reparnections\n",
      "Loxic group\n",
      "Bloschteghans theorem\n",
      "Finite-Hopgeostal category\n",
      "Errsen errorplination transform\n",
      "Abne-lays formula\n",
      "Gauss frimerity Ca\n",
      "Snav hoooten Ontle\n",
      "Lie belivid-Floqued substatistic\n",
      "Pochnetic\n",
      "Schareff–Young inequality\n",
      "Sumpay of a curve\n",
      "Borson, Sarge problem (analytic function of\n",
      "Wild invariant\n",
      "Abe\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "print(sample(model, starting_str='Algebraic', scale_factor=0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf854ed2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
